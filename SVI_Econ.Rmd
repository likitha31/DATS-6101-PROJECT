---
title: "Svi_econ"
author: "pauline"
date: "2023-11-29"
output: html_document
---

```{r Load the data file, echo=T, results='hide'}
SVI_Data <- read.csv("SVI_2020_US.csv")
head(SVI_Data)

econ <- read.csv("ACSDT5Y2020.B19013-Data.csv")
head(econ)

```

#subset to CA tracts

```{r CA tracts, echo=T, results='hide'}
Clean_data <- subset(SVI_Data, select = c(ST,STATE,ST_ABBR,STCNTY,COUNTY,FIPS,LOCATION,AREA_SQMI,EPL_POV150,	EPL_UNEMP,	EPL_HBURD,	EPL_NOHSDP,	EPL_UNINSUR,	SPL_THEME1,	RPL_THEME1,	EPL_AGE65,	EPL_AGE17,	EPL_DISABL,	EPL_SNGPNT,	EPL_LIMENG,	SPL_THEME2,	RPL_THEME2,	EPL_MINRTY,	SPL_THEME3,	RPL_THEME3, E_MINRTY, EP_HISP, EP_ASIAN, EP_AIAN, EPL_MUNIT,	EPL_MOBILE,	EPL_CROWD,	EPL_NOVEH,	EPL_GROUPQ,	SPL_THEME4,	RPL_THEME4,	SPL_THEMES,	RPL_THEMES, E_AGE65, EP_POV150, EP_AGE65, EP_NOHSDP
) )

CA_SVI <- subset(Clean_data, ST_ABBR == "CA")

CA_SVI <- subset(CA_SVI,  RPL_THEMES!= -999 )
CA_SVI <- subset(CA_SVI,  RPL_THEME1!= -999 )
CA_SVI <- subset(CA_SVI,  RPL_THEME2!= -999 )
CA_SVI <- subset(CA_SVI,  RPL_THEME3!= -999 )
CA_SVI <- subset(CA_SVI,  RPL_THEME4!= -999 )
```



#remove first row of headers 

```{r}
#clean data for economic
econ <- subset(econ, select = c(GEO_ID, NAME,B19013_001E))
econ <- econ[-c(1, 2), ] 
head(econ)
```

#rename columns

```{r}
names_to_change <- c("GEO_ID", "NAME", "B19013_001E")
new_names <- c("GEO_ID", "tract", "income")

econ <- setNames(econ, new_names)
```

# edit GEO_ID to isolate just the number after 1400000US06001400100

```{r}
# Assuming 'econ' is your data frame
econ$GEO_ID <- sub(".*US0*(\\d+)", "\\1", econ$GEO_ID)

```

#now join the datasets based on GEO_ID
```{r}
# Assuming 'econ' and 'CA_SVI' are your data frames
svi_econ <- merge(econ, CA_SVI, by.x = "GEO_ID", by.y = "FIPS", all.x = TRUE, all.y = TRUE)

```

#count how many NA 
```{r}
total_na_count <- sum(is.na(svi_econ))
print(total_na_count)
#remove NA

svi_econ <- na.omit(svi_econ)
svi_econ <- svi_econ[svi_econ$income != "-", , drop = FALSE]


#leaves us with 9001 rows 
str(svi_econ$income)
svi_econ$income <- as.numeric(as.character(svi_econ$income))
total_na_count <- sum(is.na(svi_econ))
print(total_na_count)
svi_econ <- na.omit(svi_econ)
#8956

```

#plot histogram of income variable 
```{r}
# Assuming 'merged_data' is your data frame and 'B19013_001E' is the income variable
library(ggplot2)

ggplot(svi_econ, aes(x = income)) +
  geom_histogram(binwidth = 1000, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Income Histogram", x = "Income", y = "Frequency")

#data is skewed right 
```

```{r}
# Assuming 'merged_data' is your data frame and 'B19013_001E' is the income variable
library(ggplot2)

ggplot(svi_econ, aes(x = RPL_THEMES)) +
  geom_histogram(binwidth = 0.01, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "RPL_THEMES Histogram", x = "Income", y = "Frequency")

#data is skewed left  


```
```{r}
library(ggplot2)

ggplot(svi_econ, aes(x = "", y = RPL_THEMES)) +
  geom_boxplot(fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Boxplot of SVI Score", x = "", y = "Overall SVI")

```
lets look at income on a map
```{r}
#for mapping, convert CA_SVI to a Simple Features (map object)
library(sf)
library(tigris)
library(dplyr)
library(viridis)

#Load 2020 Census Tract shapefile for California
ca_tracts <- tracts(state = "CA", year = 2020)

# Assuming svi_econ is your data frame
ca_tracts$GEOID <- sub("^\\d", "", ca_tracts$GEOID)

#Join CA_SVI and ca_tracts based on FIPS and GEOID
svi_econ_map <- inner_join(svi_econ, ca_tracts, by = c("GEO_ID" = "GEOID"))

econmap <- st_as_sf(svi_econ_map)
#!!! CHANGE COLOR RAMP
map1 = 
ggplot(data = econmap) +
  geom_sf(aes(fill = income)) +
  labs(title = "Income") +
  theme_void()
map1

```



ok but now lets also see what other features are relevant to us in this dataset in predicting svi 

first lets select the variables we should look at/consider because not all of them are relevant + processing takes a while..?


```{r}
# Assuming svi_econ is your data frame
# Assuming RPL_THEMES is the variable you want to include in the correlation matrix

# Load the dplyr package
library(dplyr)

# Convert selected columns to numeric
svi_select <- mutate_all(svi_econ, as.numeric)

# Drop columns with NA values
svi_select <- svi_select %>%
  select(everything(), -where(~any(is.na(.))))

# Create the correlation matrix
cor_matrix <- cor(svi_select, use = "complete.obs")

# Print the correlation matrix
print(cor_matrix)


# Variable of interest
target_variable <- "RPL_THEMES"

# Extract the correlations with the target variable
cor_with_target <- cor_matrix[target_variable, ]

# Select variables with high correlation (you can adjust the threshold)
high_correlation_vars <- names(cor_with_target[abs(cor_with_target) > 0.70])

# Print the variables with high correlation
print(high_correlation_vars)


```
further narrow down

```{r}
library(dplyr)

# Assuming svi_econ is your data frame
predict_svi <- svi_econ %>%
  select(RPL_THEMES, income, EPL_POV150, EPL_HBURD, EPL_NOHSDP)
```


```{r}
# Fit a linear regression model
model <- lm(RPL_THEMES ~ ., data = predict_svi)

# Check the distribution of residuals
residuals <- residuals(model)

# Residual Plot
par(mfrow = c(2, 2))
plot(model)

# Q-Q Plot
qqnorm(residuals)
qqline(residuals)


# Kernel Density Plot
plot(density(residuals))

```


https://wavedatalab.github.io/machinelearningwithr/post4.html

```{r}
library(car)

model_test <- glm(RPL_THEMES ~ ., data = predict_svi)
summary(model_test)

data.frame(vif(model_test))


```

VIF all good

now lets to stepwise 



```{r}

# Create the initial model
initial_model2 <- lm(RPL_THEMES ~ income + EPL_POV150 +EPL_UNINSUR +EPL_AGE65 +EPL_AGE17+ EPL_DISABL+ EPL_SNGPNT +EPL_LIMENG + EPL_MINRTY +EPL_UNEMP + EPL_HBURD + EPL_NOHSDP, data = svi_econ)

# Perform stepwise selection (both directions)
stepwise_model2 <- step(initial_model2, direction = "both")

# Display the summary of the selected model
summary(stepwise_model2)

```

```{r}
# Assuming svi_econ is your data frame
selected_columns <- c("RPL_THEMES", "income", "EPL_POV150", "EPL_UNINSUR", 
                       "EPL_AGE65", "EPL_AGE17", "EPL_DISABL", 
                       "EPL_SNGPNT", "EPL_LIMENG", "EPL_MINRTY", 
                       "EPL_UNEMP", "EPL_HBURD", "EPL_NOHSDP")

# Filter the data frame
trim_svi <- svi_econ %>%
  select(all_of(selected_columns))

# Load the glmnet package
library(glmnet)

# Prepare data
x <- model.matrix(RPL_THEMES ~ . - 1, data = trim_svi)
y <- svi_econ$RPL_THEMES

# Fit LASSO regression model
fit_lasso <- cv.glmnet(x, y, alpha = 1)

# Display coefficients of the LASSO model
coef_lasso <- coef(fit_lasso, s = fit_lasso$lambda.min)
print(coef_lasso)

# Fit Ridge regression model
fit_ridge <- cv.glmnet(x, y, alpha = 0)

# Display coefficients of the Ridge model
coef_ridge <- coef(fit_ridge, s = fit_ridge$lambda.min)
print(coef_ridge)

```
```{r}
# Assuming fit_lasso is your cv.glmnet object
optimal_lambda <- fit_lasso$lambda.min
coefficients_at_optimal_lambda <- coef(fit_lasso, s = optimal_lambda)
print(coefficients_at_optimal_lambda)

```
Income: The coefficient is -3.103e-07. This suggests that a one-unit increase in income is associated with a decrease of approximately 3.103e-07 units in the response variable, holding other variables constant.

EPL_POV150: The coefficient is 0.1949. This suggests that a one-unit increase in EPL_POV150 is associated with an increase of approximately 0.1949 units in the response variable, holding other variables constant.

EPL_UNINSUR: The coefficient is 0.1209. This suggests that a one-unit increase in EPL_UNINSUR is associated with an increase of approximately 0.1209 units in the response variable, holding other variables constant.

EPL_AGE65: The coefficient is 0.0933. This suggests that a one-unit increase in EPL_AGE65 is associated with an increase of approximately 0.0933 units in the response variable, holding other variables constant.

EPL_AGE17: The coefficient is 0.0216. This suggests that a one-unit increase in EPL_AGE17 is associated with an increase of approximately 0.0216 units in the response variable, holding other variables constant.

EPL_DISABL: The coefficient is 0.1503. This suggests that a one-unit increase in EPL_DISABL is associated with an increase of approximately 0.1503 units in the response variable, holding other variables constant.

EPL_SNGPNT: The coefficient is 0.1396. This suggests that a one-unit increase in EPL_SNGPNT is associated with an increase of approximately 0.1396 units in the response variable, holding other variables constant.

EPL_LIMENG: The coefficient is 0.1879. This suggests that a one-unit increase in EPL_LIMENG is associated with an increase of approximately 0.1879 units in the response variable, holding other variables constant.

EPL_MINRTY: The coefficient is 0.1249. This suggests that a one-unit increase in EPL_MINRTY is associated with an increase of approximately 0.1249 units in the response variable, holding other variables constant.

EPL_UNEMP: The coefficient is 0.0956. This suggests that a one-unit increase in EPL_UNEMP is associated with an increase of approximately 0.0956 units in the response variable, holding other variables constant.

EPL_HBURD: The coefficient is 0.1938. This suggests that a one-unit increase in EPL_HBURD is associated with an increase of approximately 0.1938 units in the response variable, holding other variables constant.

EPL_NOHSDP: The coefficient is 0.1919. This suggests that a one-unit increase in EPL_NOHSDP is associated with an increase of approximately 0.1919 units in the response variable, holding other variables constant.

all of them are non zero....




```{r}

# Create the initial model

initial_model <- lm(RPL_THEMES ~ income + EPL_POV150 + EPL_HBURD + EPL_NOHSDP, data = predict_svi)

# Perform stepwise selection (both directions)
stepwise_model <- step(initial_model, direction = "both")

# Display the summary of the selected model
summary(stepwise_model)

```

The results of the stepwise feature selection indicate that the model with all the variables (income, EPL_POV150, EPL_HBURD, EPL_NOHSDP) is preferred as it has the lowest AIC (Akaike Information Criterion). The selected model is:

all features stay relevant.

ok so lets see if model 1 or model 2 is best. 

```{r}

# Assessment for initial_model
summary(initial_model)
plot(initial_model)

# Assessment for initial_model2
summary(initial_model2)
plot(initial_model2)

# Calculate AIC for initial_model
aic_initial_model <- AIC(initial_model)

# Calculate AIC for initial_model2
aic_initial_model2 <- AIC(initial_model2)

# Compare AIC values
cat("AIC for initial_model:", aic_initial_model, "\n")
cat("AIC for initial_model2:", aic_initial_model2, "\n")

# Choose the model with the lower AIC
if (aic_initial_model < aic_initial_model2) {
  cat("Choose initial_model\n")
} else {
  cat("Choose initial_model2\n")
}

```


AIC for initial_model: -13891.99 
AIC for initial_model2: -18331.04 
Choose initial_model2
The model with the lower AIC is considered a better fit.
```{r}
# Calculate Adjusted R-squared for initial_model
adj_rsq_initial_model <- summary(initial_model)$adj.r.squared

# Calculate Adjusted R-squared for initial_model2
adj_rsq_initial_model2 <- summary(initial_model2)$adj.r.squared

# Compare Adjusted R-squared values
cat("Adjusted R-squared for initial_model:", adj_rsq_initial_model, "\n")
cat("Adjusted R-squared for initial_model2:", adj_rsq_initial_model2, "\n")

```
Adjusted R-squared for initial_model2: 0.9065885 

model 2 is better

```{r}
# Calculate VIF for initial_model
vif_initial_model <- car::vif(initial_model)

# Calculate VIF for initial_model2
vif_initial_model2 <- car::vif(initial_model2)

# Compare VIF values
cat("VIF for initial_model:", vif_initial_model, "\n")
cat("VIF for initial_model2:", vif_initial_model2, "\n")

```

ok lets use the model 2, all of the predictors... to get into using our model with all the features to then see if we can predict SVI risk into 4 groups: 

In the CDC/ATSDR SVI Interactive Map, we classify data using quartiles (0 to .2500, .2501 to .5000, .5001 to .7500, .7501 to 1.0) and indicate that the classification goes from least vulnerable to most vulnerable. While we do not have required cutoffs for working with CDC/ATSDR SVI data, categorizing CDC/ATSDR SVI values using a quantile classification (i.e., tertiles, quartiles, quintiles, etc.) is a common approach. If you choose to categorize CDC/ATSDR SVI values, we recommend you do so appropriately based on your question of interest.

```{r}
# Create quartiles and labels
trim_svi$risk <- cut(trim_svi$RPL_THEMES, breaks = c(0, 0.25, 0.5, 0.75, 1.0), labels = c("low", "lowmed", "medhigh", "high"))

# Plot the data with quartile labels
library(ggplot2)

ggplot(trim_svi, aes(x = risk, fill = risk)) +
  geom_bar() +
  labs(title = "Bar Plot of RPL_THEMES with Quartiles", x = "Quartiles", y = "Count") +
  scale_fill_manual(values = c("low" = "lightblue", "lowmed" = "blue", "medhigh" = "darkblue", "high" = "purple"))

```



```{r}
# Install and load required packages
install.packages("rpart")
library(rpart)

set.seed(1)
svi_econ_tree <- rpart(risk ~ income + EPL_POV150 + EPL_UNINSUR + EPL_AGE65 + EPL_AGE17 + EPL_DISABL + EPL_SNGPNT + EPL_LIMENG + EPL_MINRTY + EPL_UNEMP + EPL_HBURD + EPL_NOHSDP , data=trim_svi, method="class", control = list(maxdepth = 4) )
# kyphosisfit <- rpart(Kyphosis ~ Age + Number + Start, data=kyphosis, method="class", control = {rpart.control list} )
# rpart.control(maxdepth = 30, minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, ...)
printcp(svi_econ_tree) # display the results 
plotcp(svi_econ_tree) # visualize cross-validation results 

```

```{r}
summary(svi_econ_tree) # detailed summary of splits

# plot tree 
plot(svi_econ_tree, uniform=TRUE, main="Classification Tree for svi_econ_tree")
text(svi_econ_tree, use.n=TRUE, all=TRUE, cex=.8)
```

```{r}
# Generate predictions on the training data
train_predictions <- predict(svi_econ_tree, trim_svi, type = "class")

# Create a confusion matrix
conf_matrix_train <- table(train_predictions, trim_svi$risk)

# Display the confusion matrix
conf_matrix_train


xkabledply(conf_matrix_train, "confusion matrix")

```


```{r}
loadPkg("rpart")
loadPkg("caret")

# kyphosisfit <- rpart(Kyphosis ~ Age + Number + Start, data=kyphosis, method="class", control = {rpart.control list} )
# rpart.control(maxdepth = 30, minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, ...)

# create an empty dataframe to store the results from confusion matrices
confusionMatrixResultDf = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )

for (deep in 2:6) {
   kfit  <- rpart(risk ~ income + EPL_POV150 + EPL_UNINSUR + EPL_AGE65 + EPL_AGE17 + EPL_DISABL + EPL_SNGPNT + EPL_LIMENG + EPL_MINRTY + EPL_UNEMP + EPL_HBURD + EPL_NOHSDP , data=trim_svi, method="class", control = list(maxdepth = deep) )
  # 
  cm = confusionMatrix( predict(kfit, type = "class"), reference = trim_svi[, "risk"] ) # from caret library
  # 
  cmaccu = cm$overall['Accuracy']
  # print( paste("Total Accuracy = ", cmaccu ) )
  # 
  cmt = data.frame(Depth=deep, Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  confusionMatrixResultDf = rbind(confusionMatrixResultDf, cmt)
  # print("Other metrics : ")
}

unloadPkg("caret")
```


```{r}
xkabledply(confusionMatrixResultDf, title="SVI econ Classification Trees summary with varying MaxDepth")
```

```{r}
loadPkg("rpart.plot")
rpart.plot(svi_econ_tree)
```

```{r}
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
fancyRpartPlot(svi_econ_tree)
```

