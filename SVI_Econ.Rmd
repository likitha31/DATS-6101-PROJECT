---
title: "Svi_econ"
author: "pauline"
date: "2023-11-29"
output: html_document
---

```{r Load the data file, echo=T, results='hide'}
SVI_Data <- read.csv("SVI_2020_US.csv")
head(SVI_Data)

econ <- read.csv("ACSDT5Y2020.B19013-Data.csv")
head(econ)

```

#subset to CA tracts

```{r CA tracts, echo=T, results='hide'}
Clean_data <- subset(SVI_Data, select = c(ST,STATE,ST_ABBR,STCNTY,COUNTY,FIPS,LOCATION,AREA_SQMI,EPL_POV150,	EPL_UNEMP,	EPL_HBURD,	EPL_NOHSDP,	EPL_UNINSUR,	SPL_THEME1,	RPL_THEME1,	EPL_AGE65,	EPL_AGE17,	EPL_DISABL,	EPL_SNGPNT,	EPL_LIMENG,	SPL_THEME2,	RPL_THEME2,	EPL_MINRTY,	SPL_THEME3,	RPL_THEME3, E_MINRTY, EP_HISP, EP_ASIAN, EP_AIAN, EPL_MUNIT,	EPL_MOBILE,	EPL_CROWD,	EPL_NOVEH,	EPL_GROUPQ,	SPL_THEME4,	RPL_THEME4,	SPL_THEMES,	RPL_THEMES, E_AGE65, EP_POV150, EP_AGE65, EP_NOHSDP
) )

CA_SVI <- subset(Clean_data, ST_ABBR == "CA")

CA_SVI <- subset(CA_SVI,  RPL_THEMES!= -999 )
CA_SVI <- subset(CA_SVI,  RPL_THEME1!= -999 )
CA_SVI <- subset(CA_SVI,  RPL_THEME2!= -999 )
CA_SVI <- subset(CA_SVI,  RPL_THEME3!= -999 )
CA_SVI <- subset(CA_SVI,  RPL_THEME4!= -999 )
```



#remove first row of headers 

```{r}
#clean data for economic
econ <- subset(econ, select = c(GEO_ID, NAME,B19013_001E))
econ <- econ[-c(1, 2), ] 
head(econ)
```

#rename columns

```{r}
names_to_change <- c("GEO_ID", "NAME", "B19013_001E")
new_names <- c("GEO_ID", "tract", "income")

econ <- setNames(econ, new_names)
```

# edit GEO_ID to isolate just the number after 1400000US06001400100

```{r}
# Assuming 'econ' is your data frame
econ$GEO_ID <- sub(".*US0*(\\d+)", "\\1", econ$GEO_ID)

```

#now join the datasets based on GEO_ID
```{r}
# Assuming 'econ' and 'CA_SVI' are your data frames
svi_econ <- merge(econ, CA_SVI, by.x = "GEO_ID", by.y = "FIPS", all.x = TRUE, all.y = TRUE)

```

#count how many NA 
```{r}
total_na_count <- sum(is.na(svi_econ))
print(total_na_count)
#remove NA

svi_econ <- na.omit(svi_econ)
svi_econ <- svi_econ[svi_econ$income != "-", , drop = FALSE]


#leaves us with 9001 rows 
str(svi_econ$income)
svi_econ$income <- as.numeric(as.character(svi_econ$income))
total_na_count <- sum(is.na(svi_econ))
print(total_na_count)
svi_econ <- na.omit(svi_econ)
#8956

```

#plot histogram of income variable 
```{r}
# Assuming 'merged_data' is your data frame and 'B19013_001E' is the income variable
library(ggplot2)

ggplot(svi_econ, aes(x = income)) +
  geom_histogram(binwidth = 1000, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Income Histogram", x = "Income", y = "Frequency")

#data is skewed right 
```

```{r}
# Assuming 'merged_data' is your data frame and 'B19013_001E' is the income variable
library(ggplot2)

ggplot(svi_econ, aes(x = RPL_THEMES)) +
  geom_histogram(binwidth = 0.01, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Income Histogram", x = "Income", y = "Frequency")

#data is skewed left  


```
```{r}
library(ggplot2)

ggplot(svi_econ, aes(x = "", y = RPL_THEMES)) +
  geom_boxplot(fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Boxplot of SVI Score", x = "", y = "Overall SVI")

```
In the CDC/ATSDR SVI Interactive Map, we classify data using quartiles (0 to .2500, .2501 to .5000, .5001 to .7500, .7501 to 1.0) and indicate that the classification goes from least vulnerable to most vulnerable. While we do not have required cutoffs for working with CDC/ATSDR SVI data, categorizing CDC/ATSDR SVI values using a quantile classification (i.e., tertiles, quartiles, quintiles, etc.) is a common approach. If you choose to categorize CDC/ATSDR SVI values, we recommend you do so appropriately based on your question of interest.

```{r}
#The interactive map uses these values:
# Assuming svi_econ is your data frame

# Create quartiles and labels
svi_econ$risk <- cut(svi_econ$RPL_THEMES, breaks = c(0, 0.25, 0.5, 0.75, 1.0), labels = c("low", "lowmed", "medhigh", "high"))

# Plot the data with quartile labels
library(ggplot2)

ggplot(svi_econ, aes(x = risk, fill = risk)) +
  geom_bar() +
  labs(title = "Bar Plot of RPL_THEMES with Quartiles", x = "Quartiles", y = "Count") +
  scale_fill_manual(values = c("low" = "lightblue", "lowmed" = "blue", "medhigh" = "darkblue", "high" = "purple"))

```
lets look at income on a map
```{r}
#for mapping, convert CA_SVI to a Simple Features (map object)
library(sf)
library(tigris)
library(dplyr)
library(viridis)

#Load 2020 Census Tract shapefile for California
ca_tracts <- tracts(state = "CA", year = 2020)

# Assuming svi_econ is your data frame
ca_tracts$GEOID <- sub("^\\d", "", ca_tracts$GEOID)

#Join CA_SVI and ca_tracts based on FIPS and GEOID
svi_econ_map <- inner_join(svi_econ, ca_tracts, by = c("GEO_ID" = "GEOID"))

econmap <- st_as_sf(svi_econ_map)

map1 = 
ggplot(data = econmap) +
  geom_sf(aes(fill = income)) +
  labs(title = "Income") +
  theme_void()
map1

```
Map risk by levels

```{r}
map1 = 
ggplot(data = econmap) +
  geom_sf(aes(fill = risk)) +
  labs(title = "Risk") +
  theme_void()
map1
```

Now lets do a logstic regression on income and svi 
```{r}
# Assuming svi_econ is your data frame


# Split the data into training and testing sets
set.seed(123)  # for reproducibility
train_indices <- sample(seq_len(nrow(svi_econ)), 0.8 * nrow(svi_econ))
train_data <- svi_econ[train_indices, ]
test_data <- svi_econ[-train_indices, ]

# Build logistic regression model
logit_model <- glm(risk ~ income, data = train_data, family = "binomial")

# Summary of the model
summary(logit_model)

# Predict on the test set
logit_predictions <- predict(logit_model, test_data, type = "response")

# Convert logistic regression probabilities to predicted classes
logit_predictions_class <- cut(logit_predictions, breaks = c(0, 0.25, 0.5, 0.75, 1.0), labels = c("low", "lowmed", "medhigh", "high"))

# Evaluate the model (e.g., confusion matrix, accuracy, etc.)
conf_matrix <- table(logit_predictions_class, test_data$risk)
print(conf_matrix)

```

ok but now lets also see what other features are relevant to us in this dataset in predicting svi 

first lets select the variables we should look at/consider because not all of them are relevant + processing takes a while


```{r}
# Assuming svi_econ is your data frame
# Assuming RPL_THEMES is the variable you want to include in the correlation matrix

# Load the dplyr package
library(dplyr)

# Convert selected columns to numeric
svi_select <- mutate_all(svi_econ, as.numeric)

# Drop columns with NA values
svi_select <- svi_select %>%
  select(everything(), -where(~any(is.na(.))))

# Create the correlation matrix
cor_matrix <- cor(svi_select, use = "complete.obs")

# Print the correlation matrix
print(cor_matrix)


# Variable of interest
target_variable <- "RPL_THEMES"

# Extract the correlations with the target variable
cor_with_target <- cor_matrix[target_variable, ]

# Select variables with high correlation (you can adjust the threshold)
high_correlation_vars <- names(cor_with_target[abs(cor_with_target) > 0.70])

# Print the variables with high correlation
print(high_correlation_vars)


```
further narrow down

```{r}
library(dplyr)

# Assuming svi_econ is your data frame
predict_svi <- svi_econ %>%
  select(RPL_THEMES, income, EPL_POV150, EPL_HBURD, EPL_NOHSDP, EP_POV150, EP_NOHSDP)
```



